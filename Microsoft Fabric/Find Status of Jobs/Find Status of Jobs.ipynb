{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "# Replace with your actual item ID\n",
                "item_id = \"\"  #datapipeline\n",
                "#item_id = \"\" #copyjob"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "tags": [
                    "parameters"
                ]
            },
            "id": "196c5c86-4789-4f75-87aa-27741b6522d5"
        },
        {
            "cell_type": "code",
            "source": [
                "import requests\n",
                "import json\n",
                "import sempy.fabric as fabric\n",
                "\n",
                "\n",
                "# Get the current workspace ID dynamically\n",
                "workspace_id = fabric.get_notebook_workspace_id()\n",
                "\n",
                "# Get the current user's access token\n",
                "access_token = notebookutils.credentials.getToken('pbi')\n",
                "\n",
                "# Construct the API URL Jobs Instances\n",
                "url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{item_id}/jobs/instances\"\n",
                "\n",
                "\n",
                "# Set headers\n",
                "headers = {\n",
                "    \"Authorization\": f\"Bearer {access_token}\",\n",
                "    \"Content-Type\": \"application/json\"\n",
                "}\n",
                "\n",
                "# Make the API request\n",
                "response = requests.get(url, headers=headers)\n",
                "\n",
                "\n",
                "# Check and process the response\n",
                "if response.status_code == 200:\n",
                "    data = response.json()\n",
                "    job_instances = data.get(\"value\", []) # Extract the list of job instances\n",
                "    if job_instances:\n",
                "        \n",
                "        json_lines = [json.dumps(job_instances) for job_instance in job_instances]\n",
                "        \n",
                "        df = spark.read.json(spark.sparkContext.parallelize(json_lines))\n",
                "        #print('df about to run')\n",
                "        #df.show(truncate=False)\n",
                "        #display(df)\n",
                "    else:\n",
                "        print(\"No job instances found.\")\n",
                "else:\n",
                "    print(f\"Failed to fetch job status: {response.status_code} - {response.text}\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "collapsed": false
            },
            "id": "0da9ff9f-d002-4c9f-b117-ddc6b3b153ab"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import col\n",
                "from pyspark.sql.types import TimestampType\n",
                "\n",
                "# Ensure startTimeUtc is in timestamp format\n",
                "df = df.withColumn(\"startTimeUtc\", col(\"startTimeUtc\").cast(TimestampType()))\n",
                "\n",
                "# Sort by startTimeUtc descending\n",
                "df_sorted = df.orderBy(col(\"startTimeUtc\").desc())\n",
                "\n",
                "# Filter out rows where status is 'Deduped'\n",
                "df_filtered = df_sorted.filter(col(\"status\") != \"Deduped\")\n",
                "\n",
                "# Get the first non-Deduped status\n",
                "latest_status = df_filtered.select(\"status\").first()[\"status\"]\n",
                "\n",
                "#print(f\"The latest non-Deduped status is: {latest_status}\")\n"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "815a3cc8-e93b-4679-971f-d4d41081ea57"
        },
        {
            "cell_type": "code",
            "source": [
                "notebookutils.notebook.exit(latest_status)"
            ],
            "outputs": [],
            "execution_count": null,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "34903b61-df71-47d6-9e1d-cf643983ddfc"
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "known_lakehouses": [
                    {
                        "id": ""
                    }
                ],
                "default_lakehouse": "",
                "default_lakehouse_name": "",
                "default_lakehouse_workspace_id": ""
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}